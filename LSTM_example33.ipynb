{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iakovkh/mypython/blob/main/LSTM_example33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Configuration section\n",
        "data_file_name = '/content/sample_data/NN_cut2.xlsx'  # Path to the training data file\n",
        "validate_file_name = '/content/sample_data/NN_control_02.xlsx'  # Path to the validation data file\n",
        "normalize_data = True  # Flag to indicate whether to normalize the data\n",
        "train_ratio = 0.8  # Ratio of data to be used for training\n",
        "optimizer_name = 'Adam'  # Name of the optimizer to use (options: 'SGD', 'RMSprop', 'Adagrad', 'Adam')\n",
        "loss_function_name = 'MSELoss'  # Name of the loss function to use (options: 'L1Loss', 'CrossEntropyLoss', 'MSELoss')\n",
        "activation_function_name = 'Sigmoid'  # Name of the activation function to use (options: 'Sigmoid', 'Tanh', 'ReLU')\n",
        "epochs = 15000  # Number of epochs for training\n",
        "network_structure = [(1, 3), (2, 2), (3, 1), (4, 2)]  # Structure of the neural network (layer number, number of neurons)\n",
        "display_network_structure = False  # Flag to indicate whether to display the network structure\n",
        "\n",
        "# Step I - General data preparation\n",
        "class MyMatrix:\n",
        "    def __init__(self):\n",
        "        self.data = None  # Initialize data attribute to None\n",
        "\n",
        "    def read_data_from_file(self, file_name):\n",
        "        self.data = pd.read_excel(file_name)  # Read data from an Excel file\n",
        "\n",
        "    def normalize_data(self):\n",
        "        scaler = MinMaxScaler()  # Create a MinMaxScaler object\n",
        "        self.data = pd.DataFrame(scaler.fit_transform(self.data), columns=self.data.columns)  # Normalize the data\n",
        "\n",
        "    def display_console(self, column=None):\n",
        "        if column:\n",
        "            print(self.data[column])  # Print a specific column of the data\n",
        "        else:\n",
        "            print(self.data)  # Print the entire data\n",
        "\n",
        "    def display_visual(self, column=None):\n",
        "        if column:\n",
        "            self.data[column].plot()  # Plot a specific column of the data\n",
        "            plt.show()  # Display the plot\n",
        "        else:\n",
        "            self.data.plot(subplots=True)  # Plot all columns of the data in separate subplots\n",
        "            plt.show()  # Display the plots\n",
        "\n",
        "# Step II – Read and prepare data\n",
        "MyData = MyMatrix()  # Create an instance of MyMatrix\n",
        "MyData.read_data_from_file(data_file_name)  # Read the training data from the file\n",
        "\n",
        "if normalize_data:\n",
        "    MyData.normalize_data()  # Normalize the data if the flag is set\n",
        "\n",
        "train_size = int(len(MyData.data) * train_ratio)  # Calculate the size of the training data\n",
        "train_data = MyData.data.iloc[:train_size]  # Split the data into training data\n",
        "test_data = MyData.data.iloc[train_size:]  # Split the data into testing data\n",
        "\n",
        "# Step III – Design the neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, network_structure, activation_function):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        layers = []  # Initialize a list to hold the layers of the network\n",
        "        for i, (layer_num, neurons) in enumerate(network_structure):\n",
        "            if i == 0:\n",
        "                layers.append(nn.Linear(input_size, neurons))  # Add the first layer with input size\n",
        "            else:\n",
        "                layers.append(nn.Linear(network_structure[i-1][1], neurons))  # Add subsequent layers\n",
        "            if i < len(network_structure) - 1:\n",
        "                layers.append(activation_function())  # Add activation function between layers\n",
        "        self.network = nn.Sequential(*layers)  # Create a sequential container of layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)  # Define the forward pass\n",
        "\n",
        "input_size = train_data.shape[1] - 2  # Determine the input size for the network\n",
        "activation_function = getattr(nn, activation_function_name)  # Get the activation function\n",
        "model = NeuralNetwork(input_size, network_structure, activation_function)  # Create the neural network model\n",
        "\n",
        "if display_network_structure:\n",
        "    print(\"Network Structure:\")\n",
        "    for i, (layer_num, neurons) in enumerate(network_structure):\n",
        "        print(f\"Layer {layer_num}: {neurons} neurons\")  # Print the structure of the network\n",
        "    print(\"Training Data:\")\n",
        "    print(train_data)  # Print the training data\n",
        "    print(\"Testing Data:\")\n",
        "    print(test_data)  # Print the testing data\n",
        "\n",
        "# Training the network\n",
        "optimizer = getattr(optim, optimizer_name)(model.parameters())  # Get the optimizer\n",
        "loss_function = getattr(nn, loss_function_name)()  # Get the loss function\n",
        "\n",
        "train_inputs = torch.tensor(train_data.iloc[:, 2:].values, dtype=torch.float32)  # Prepare training inputs\n",
        "train_outputs = torch.tensor(train_data.iloc[:, :2].values, dtype=torch.float32)  # Prepare training outputs\n",
        "test_inputs = torch.tensor(test_data.iloc[:, 2:].values, dtype=torch.float32)  # Prepare testing inputs\n",
        "test_outputs = torch.tensor(test_data.iloc[:, :2].values, dtype=torch.float32)  # Prepare testing outputs\n",
        "\n",
        "losses = []  # Initialize a list to store the loss values\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    outputs = model(train_inputs)  # Forward pass\n",
        "    loss = loss_function(outputs, train_outputs)  # Calculate the loss\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Update the weights\n",
        "    losses.append(loss.item())  # Store the loss value\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item()}\")  # Print the loss every 50 epochs\n",
        "\n",
        "plt.plot(range(epochs), losses)  # Plot the loss values over epochs\n",
        "plt.xlabel('Epochs')  # Label for x-axis\n",
        "plt.ylabel('Loss')  # Label for y-axis\n",
        "plt.title('Training Loss')  # Title of the plot\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Step IV - Forecasting results\n",
        "MyData.read_data_from_file(validate_file_name)  # Read the validation data from the file\n",
        "if normalize_data:\n",
        "    MyData.normalize_data()  # Normalize the validation data if the flag is set\n",
        "\n",
        "validate_inputs = torch.tensor(MyData.data.iloc[:, 2:].values, dtype=torch.float32)  # Prepare validation inputs\n",
        "predictions = model(validate_inputs).detach().numpy()  # Make predictions and convert to numpy array\n",
        "\n",
        "output_df = pd.DataFrame(predictions, columns=['Predicted_Column1', 'Predicted_Column2'])  # Create a DataFrame for predictions\n",
        "output_df.to_excel('output.xlsx', index=False)  # Save the predictions to an Excel file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1w3qb9b_WfW",
        "outputId": "f92d143d-aa90-4eae-d56e-24d45c91f3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/15000, Loss: 0.46077749133110046\n",
            "Epoch 50/15000, Loss: 0.3847995698451996\n",
            "Epoch 100/15000, Loss: 0.3283790946006775\n",
            "Epoch 150/15000, Loss: 0.2852981388568878\n",
            "Epoch 200/15000, Loss: 0.25215452909469604\n",
            "Epoch 250/15000, Loss: 0.22767901420593262\n",
            "Epoch 300/15000, Loss: 0.21066753566265106\n",
            "Epoch 350/15000, Loss: 0.19963161647319794\n",
            "Epoch 400/15000, Loss: 0.19297677278518677\n",
            "Epoch 450/15000, Loss: 0.18925058841705322\n",
            "Epoch 500/15000, Loss: 0.1873096376657486\n",
            "Epoch 550/15000, Loss: 0.18636344373226166\n",
            "Epoch 600/15000, Loss: 0.18592458963394165\n",
            "Epoch 650/15000, Loss: 0.18572168052196503\n",
            "Epoch 700/15000, Loss: 0.18561576306819916\n",
            "Epoch 750/15000, Loss: 0.18553954362869263\n",
            "Epoch 800/15000, Loss: 0.18545952439308167\n",
            "Epoch 850/15000, Loss: 0.1853555291891098\n",
            "Epoch 900/15000, Loss: 0.18521089851856232\n",
            "Epoch 950/15000, Loss: 0.18500745296478271\n",
            "Epoch 1000/15000, Loss: 0.184719055891037\n",
            "Epoch 1050/15000, Loss: 0.18431386351585388\n",
            "Epoch 1100/15000, Loss: 0.18379545211791992\n",
            "Epoch 1150/15000, Loss: 0.18319429457187653\n",
            "Epoch 1200/15000, Loss: 0.18252800405025482\n",
            "Epoch 1250/15000, Loss: 0.18180157244205475\n",
            "Epoch 1300/15000, Loss: 0.18100813031196594\n",
            "Epoch 1350/15000, Loss: 0.18013359606266022\n",
            "Epoch 1400/15000, Loss: 0.17916016280651093\n",
            "Epoch 1450/15000, Loss: 0.17806784808635712\n",
            "Epoch 1500/15000, Loss: 0.17683573067188263\n",
            "Epoch 1550/15000, Loss: 0.17544233798980713\n",
            "Epoch 1600/15000, Loss: 0.17386604845523834\n",
            "Epoch 1650/15000, Loss: 0.17208589613437653\n",
            "Epoch 1700/15000, Loss: 0.17008307576179504\n",
            "Epoch 1750/15000, Loss: 0.16784359514713287\n",
            "Epoch 1800/15000, Loss: 0.1653611809015274\n",
            "Epoch 1850/15000, Loss: 0.16264061629772186\n",
            "Epoch 1900/15000, Loss: 0.15970578789710999\n",
            "Epoch 1950/15000, Loss: 0.1566150188446045\n",
            "Epoch 2000/15000, Loss: 0.15346448123455048\n",
            "Epoch 2050/15000, Loss: 0.1503640115261078\n",
            "Epoch 2100/15000, Loss: 0.14741185307502747\n",
            "Epoch 2150/15000, Loss: 0.1446782946586609\n",
            "Epoch 2200/15000, Loss: 0.14219218492507935\n",
            "Epoch 2250/15000, Loss: 0.13992871344089508\n",
            "Epoch 2300/15000, Loss: 0.13780681788921356\n",
            "Epoch 2350/15000, Loss: 0.13575546443462372\n",
            "Epoch 2400/15000, Loss: 0.1338127851486206\n",
            "Epoch 2450/15000, Loss: 0.13202345371246338\n",
            "Epoch 2500/15000, Loss: 0.13036994636058807\n",
            "Epoch 2550/15000, Loss: 0.12882281839847565\n",
            "Epoch 2600/15000, Loss: 0.12736256420612335\n",
            "Epoch 2650/15000, Loss: 0.1259801983833313\n",
            "Epoch 2700/15000, Loss: 0.12467388063669205\n",
            "Epoch 2750/15000, Loss: 0.12344519048929214\n",
            "Epoch 2800/15000, Loss: 0.12229697406291962\n",
            "Epoch 2850/15000, Loss: 0.12123201787471771\n",
            "Epoch 2900/15000, Loss: 0.12025114893913269\n",
            "Epoch 2950/15000, Loss: 0.11935218423604965\n",
            "Epoch 3000/15000, Loss: 0.1185302808880806\n",
            "Epoch 3050/15000, Loss: 0.1177787184715271\n",
            "Epoch 3100/15000, Loss: 0.11708922684192657\n",
            "Epoch 3150/15000, Loss: 0.11645270138978958\n",
            "Epoch 3200/15000, Loss: 0.11585985869169235\n",
            "Epoch 3250/15000, Loss: 0.11530183255672455\n",
            "Epoch 3300/15000, Loss: 0.11477062851190567\n",
            "Epoch 3350/15000, Loss: 0.11425944417715073\n",
            "Epoch 3400/15000, Loss: 0.11376282572746277\n",
            "Epoch 3450/15000, Loss: 0.1132763996720314\n",
            "Epoch 3500/15000, Loss: 0.11279667168855667\n",
            "Epoch 3550/15000, Loss: 0.11232060194015503\n",
            "Epoch 3600/15000, Loss: 0.11184524744749069\n",
            "Epoch 3650/15000, Loss: 0.11136707663536072\n",
            "Epoch 3700/15000, Loss: 0.11088144034147263\n",
            "Epoch 3750/15000, Loss: 0.1103820726275444\n",
            "Epoch 3800/15000, Loss: 0.10986297577619553\n",
            "Epoch 3850/15000, Loss: 0.10932443290948868\n",
            "Epoch 3900/15000, Loss: 0.10877422243356705\n",
            "Epoch 3950/15000, Loss: 0.10821544378995895\n",
            "Epoch 4000/15000, Loss: 0.1076444759964943\n",
            "Epoch 4050/15000, Loss: 0.10706384479999542\n",
            "Epoch 4100/15000, Loss: 0.10648524761199951\n",
            "Epoch 4150/15000, Loss: 0.10591871291399002\n",
            "Epoch 4200/15000, Loss: 0.10536881536245346\n",
            "Epoch 4250/15000, Loss: 0.10483690351247787\n",
            "Epoch 4300/15000, Loss: 0.10432315617799759\n",
            "Epoch 4350/15000, Loss: 0.10382730513811111\n",
            "Epoch 4400/15000, Loss: 0.1033487468957901\n",
            "Epoch 4450/15000, Loss: 0.10288667678833008\n",
            "Epoch 4500/15000, Loss: 0.10244034975767136\n",
            "Epoch 4550/15000, Loss: 0.10200921446084976\n",
            "Epoch 4600/15000, Loss: 0.10159306973218918\n",
            "Epoch 4650/15000, Loss: 0.10119180381298065\n",
            "Epoch 4700/15000, Loss: 0.10080515593290329\n",
            "Epoch 4750/15000, Loss: 0.10043243318796158\n",
            "Epoch 4800/15000, Loss: 0.10007267445325851\n",
            "Epoch 4850/15000, Loss: 0.09972460567951202\n",
            "Epoch 4900/15000, Loss: 0.09938707202672958\n",
            "Epoch 4950/15000, Loss: 0.09905920922756195\n",
            "Epoch 5000/15000, Loss: 0.09874056279659271\n",
            "Epoch 5050/15000, Loss: 0.09843110293149948\n",
            "Epoch 5100/15000, Loss: 0.09813102334737778\n",
            "Epoch 5150/15000, Loss: 0.09784045070409775\n",
            "Epoch 5200/15000, Loss: 0.09755931794643402\n",
            "Epoch 5250/15000, Loss: 0.09728723019361496\n",
            "Epoch 5300/15000, Loss: 0.09702352434396744\n",
            "Epoch 5350/15000, Loss: 0.0967673808336258\n",
            "Epoch 5400/15000, Loss: 0.09651793539524078\n",
            "Epoch 5450/15000, Loss: 0.0962742269039154\n",
            "Epoch 5500/15000, Loss: 0.09603546559810638\n",
            "Epoch 5550/15000, Loss: 0.09580081701278687\n",
            "Epoch 5600/15000, Loss: 0.09556959569454193\n",
            "Epoch 5650/15000, Loss: 0.09534122049808502\n",
            "Epoch 5700/15000, Loss: 0.09511515498161316\n",
            "Epoch 5750/15000, Loss: 0.09489098936319351\n",
            "Epoch 5800/15000, Loss: 0.0946684181690216\n",
            "Epoch 5850/15000, Loss: 0.09444724768400192\n",
            "Epoch 5900/15000, Loss: 0.09422733634710312\n",
            "Epoch 5950/15000, Loss: 0.09400869160890579\n",
            "Epoch 6000/15000, Loss: 0.09379134327173233\n",
            "Epoch 6050/15000, Loss: 0.0935753881931305\n",
            "Epoch 6100/15000, Loss: 0.09336099773645401\n",
            "Epoch 6150/15000, Loss: 0.09314832091331482\n",
            "Epoch 6200/15000, Loss: 0.09293746948242188\n",
            "Epoch 6250/15000, Loss: 0.09272860735654831\n",
            "Epoch 6300/15000, Loss: 0.09252182394266129\n",
            "Epoch 6350/15000, Loss: 0.09231719374656677\n",
            "Epoch 6400/15000, Loss: 0.09211476892232895\n",
            "Epoch 6450/15000, Loss: 0.09191452711820602\n",
            "Epoch 6500/15000, Loss: 0.091716468334198\n",
            "Epoch 6550/15000, Loss: 0.09152054786682129\n",
            "Epoch 6600/15000, Loss: 0.09132670611143112\n",
            "Epoch 6650/15000, Loss: 0.09113483875989914\n",
            "Epoch 6700/15000, Loss: 0.09094490855932236\n",
            "Epoch 6750/15000, Loss: 0.09075680375099182\n",
            "Epoch 6800/15000, Loss: 0.09057046473026276\n",
            "Epoch 6850/15000, Loss: 0.09038585424423218\n",
            "Epoch 6900/15000, Loss: 0.09020289033651352\n",
            "Epoch 6950/15000, Loss: 0.09002156555652618\n",
            "Epoch 7000/15000, Loss: 0.08984185010194778\n",
            "Epoch 7050/15000, Loss: 0.08966374397277832\n",
            "Epoch 7100/15000, Loss: 0.08948725461959839\n",
            "Epoch 7150/15000, Loss: 0.08931241929531097\n",
            "Epoch 7200/15000, Loss: 0.08913921564817429\n",
            "Epoch 7250/15000, Loss: 0.0889677107334137\n",
            "Epoch 7300/15000, Loss: 0.088797926902771\n",
            "Epoch 7350/15000, Loss: 0.08862989395856857\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}